1223
    - LJG: 개고생 => nova instances테이블의 hostname 컬럼값이 이상함 
        -- instances 테이블의 hostname 컬럼은 jingoo_server라면 Vm 이름을 넣으면 
        -- jingoo-server라고 표시됨 윈인은 몰라.. 따라서 display_name을 사용해야 함.
      
       
1203
    - openstack 에서 cnode 제거방법
    
    단순무식::
        -- delete from compute_node_stats where compute_node_id=(select cn.id from compute_nodes cn, services s where cn.service_id=s.id and s.host='hostname');
        -- delete from nova.compute_nodes where service_id=(select id from nova.services where host='cnode03');
        -- delete from nova.services where host='cnode03';
    
    삭제는 말고 단순히 disable
        nova-manage service disable --host=cnode03 --service=nova-compute
             
1125
    LJG(notice!!) 특정값에 '_"를 넣은 경우 cli와 DB에 들어간 vm 이름이 다르다.       
    예) 데이터베이스에 저장된 VM 이름 => jingoo-utm
        CLI nova list 로 나온 VM 이름 => jingoo_utm
        
        원래 생성할 때 "jingoo_utm"이었는데 왜 데이터베이스에 이상하게 값이 들어갈까?
        
    상세예) 
        데이터베이스에 저장된 VM 이름
            hostname     
            -------------
            jingoo-utm   
            active-utm   
            standby-utm  
            gmgmt-vm   
    
    CLI로 출력한 VM 이름
        root@controller:~/openstack/test_senario/admin_prepare# nova list --all-tenants
        +--------------------------------------+-------------+---------+------------+-------------+--------------------------------------------------------------------------------------------------------------+
        | ID                                   | Name        | Status  | Task State | Power State | Networks                                                                                                     |
        +--------------------------------------+-------------+---------+------------+-------------+--------------------------------------------------------------------------------------------------------------+
        | ab463d54-ad00-4c22-85d8-b2b064278c54 | active_UTM  | ACTIVE  | -          | Running     | global_mgmt_net=10.10.10.103; red_shared_public_net=221.145.180.99; g_net=192.168.10.13; o_net=192.168.20.13 |
        | 5be8ed0f-94f6-47f4-b9d6-489b3035449d | gmgmt_vm    | ACTIVE  | -          | Running     | global_mgmt_net=10.10.10.107                                                                                 |
        | 0b73c6fa-bacb-46dd-8785-9482d61d1ba1 | jingoo_utm  | SHUTOFF | -          | Shutdown    | global_mgmt_net=10.10.10.217; jingoo_green_net=192.168.0.1; jingoo_orange_net=192.168.0.225                  |        
        | fa81e18a-47f0-46ad-b6dc-c980927697eb | standby_UTM | ACTIVE  | -          | Running     | global_mgmt_net=10.10.10.104; red_shared_public_net=221.145.180.97; g_net=192.168.10.14; o_net=192.168.20.14 |
        | 7fbc5210-da96-4af8-9cf0-7b23962fb1e3 | test11      | ACTIVE  | -          | Running     | public_net=221.145.180.93                                                                                    |
        +--------------------------------------+-------------+---------+------------+-------------+--------------------------------------------------------------------------------------------------------------+

1124

    http://sir.co.kr/bbs/board.php?bo_table=pg_tip&wr_id=11987
    
    mysql 접속시
    Lost connection to MySQL server at 'reading initial communication packet', system error: 110
    같은 에러가 자주 나는데 왜 그런지 혹시 아시는 분 계신지요?
    
    => 방화벽을 정확히 잘 모르긴 한데요, 제 경험을 토대로 얘기드리자면.. 
    방화벽 설정을 잘못하는경우에 아예 안되는것이 아니라, 느리게 패킷이 전송되는 경우가 있습니다.. 
    무슨 설정인지 모르겠는데, 가끔 방화벽 설정을 잘못해놔서, ftp나 ssh가 엄청 느려지거든요. 
    그럴때, 서버 관리자에게 요청하면 다시 빠르게 해주더라구요. 
    위의 경우도 패킷이 느리게 전송되어서 강제로 서버가 끊어버린게 아닌가 싶습니다.

    => 
    방화벽을 새로 설정하니 문제가 없어졋습니다.
    아무래도 서버 관리자가 방화벽을 잘 못 셑팅해서 발생한 문제 였던것 같습니다. 
    많은 분들 답변 감사드립니다. 고맙습니다.
    
    => 남곤이가 보안을 위해 서초에 ssh만 접속하도록 방화벽에 acl을 걸어놓아 발생한 문제였슴
     
1116
    - 생성된 VM에 novnc 웹으로 접근하는 방법
      
      1. nova list 로 생성된 VM ID를 구한다.
      2. 리턴된 url을 추출한다.
      3. 추출된 url을 이용하여 웹팝업창을 하나 띄우고 url을 실행한다.

    
    root@controller:/var/log/openvswitch# nova get-vnc-console 86158a1d-970f-4beb-89fa-37918a36af94 novnc
    +-------+--------------------------------------------------------------------------------------+
    | Type  | Url                                                                                  |
    +-------+--------------------------------------------------------------------------------------+
    | novnc | http://211.224.204.144:6080/vnc_auto.html?token=0653a1da-5c56-4c51-8e0e-7a6cf2636fd4 |
    +-------+--------------------------------------------------------------------------------------+

1115
    - LJG: 심각한 오류, troubleshoot
    사업화를 위해 controller, cnode01, code02 ...  등으로 구성해서 운용할 때
    vm 생성이 안되고 계속 build 상태에 hang 걸리는 상황이 자주 출현하고 있다.
    
    - 대충 quick fix를 해서 성공한 경우는 대부분 cnode를 재시작한 경우이다.
      이는 controller와 cnode 사이에 sync가 깨질 경우 오픈스택 자체가 정상동작하지 않는다는 얘기이다.
      이게 심각한 문제인 것은 기본적으로 cnode에서 서비스가 동작할 경우
      controller를 재시작하는 것은 서비스에 영향을 미치지 않으나 
      cnode가 재시작하는 것은 고객의 서비스에 영향을 미친다는 의미이므로 
      정밀탐색해서 원인을 해결해야 한다.  
     
    - public network ip pool 관리
      기본적으로 floating ip 대역과 UTM에서 사용하는 red 대역은 
      라우팅이 가능하도록 다른 대역을 사용해야 한다.
      예를 들어 하나의 VM에 red ip를 사용함과 동시에 floating ip를 할당한 상황에서
      우리가 floating IP로 해당 VM에 접속하면 그 VM은 tcp ack를 
      red ip 대역을 통해서 응답하므로 floating ip를 통해 통신하려는
      클라이언트는 tcp ack를 받지 못해서 접속이 실패하게 된다.   

    - openstack rabbitMQ Web Management 방법  
       # /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management
       # service rabbitmq-server restart
       # http://localhost:55672/ 접속    
    
1110
    - pexpect에서 로컬에 원격서버에 대한 인증키가 존재하는 경우 
      암호입력 없이 자동으로 실행되면 pexpect.EOF 가 리턴되므로
      expect에 pexpect.EOF 파라미터를 추가해 주어야 함
      
      기존:
      i = child.expect([pexpect.TIMEOUT, ssh_newkey, 'password: '], timeout=self.timeout)
      
      수정:
      i = child.expect([pexpect.TIMEOUT, ssh_newkey, 'password: ', pexpect.EOF], timeout=self.timeout)
     
1105
    - rsyslog 설정
    
      :: controller에 개별 cnode의 로그데이터를 수집
      
        디렉토리 & 파일 구조
        
            all_host_openstack.log  -> 모든 호스트에서 수집된 오픈스택 관련 로그들
            all_host_syslog.log     -> 모든 호스트에서 수집된 syslog
            cnode01
                openstack.log   -> cnode01 호스트에서 수집된 오픈스택 관련 로그들
                syslog.log      -> cnode01 호스트에서 수집된 syslog
            cnode02
                openstack.log   -> cnode02 호스트에서 수집된 오픈스택 관련 로그들
                syslog.log      -> cnode02 호스트에서 수집된 syslog            
            controller
                openstack.log   -> controller 호스트에서 수집된 오픈스택 관련 로그들
                syslog.log      -> controller 호스트에서 수집된 syslog
        
        
        - server side
            1. /etc/rsyslog.conf 편집 -> 동적으로 로그파일을 생성하여 원격로그를 저장할 권한 설정
            원본
            $PrivDropToUser syslog
            $PrivDropToGroup syslog
            변경본
            $PrivDropToUser adm
            $PrivDropToGroup adm
        
            2. /etc/rsyslog.d 디렉토리에 server.conf 파일 생성 
                -> /etc/rsyslog.conf 파일에 이미 rsyslog 기본설정은 되어 있슴
                   여기는 로그파일 포맷만 설정                   
            
            # Create logging templates for all openstack host logs
            $template AllHostOpenstackLogFile,"/var/log/rsyslog/all_host_openstack.log"
            # Save all host openstack log
            local0.* ?AllHostOpenstackLogFile
            
            # Create logging templates for all host syslogs
            $template AllHostSyslogFile,"/var/log/rsyslog/all_host_syslog.log"
            # Save all host syslog
            *.*;local0.none ?AllHostSyslogFile            
            
            # Create logging templates for each host nova
            $template HostOpenStackLogFile,"/var/log/rsyslog/%HOSTNAME%/openstack.log"
            # Save each host openstack log
            local0.* ?HostOpenStackLogFile            
            
            # Create logging templates for each host syslog
            $template HostSyslogFile,"/var/log/rsyslog/%HOSTNAME%/syslog.log"
            # Save each host syslog
            *.*;local0.none ?HostSyslogFile
            & ~
        
        - server side logrotate.d 설정
            
            vi /etc/logrotate.d/all_host_openstack
            /var/log/rsyslog/all_host_openstack.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }
            
            vi /etc/logrotate.d/all_host_syslog
            /var/log/rsyslog/all_host_syslog.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }
            
            vi /etc/logrotate.d/cnode01_host_openstack
            /var/log/rsyslog/cnode01/openstack.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }
            
            vi /etc/logrotate.d/cnode01_syslog
            /var/log/rsyslog/cnode01/syslog.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }

            vi /etc/logrotate.d/cnode02_host_openstack
            /var/log/rsyslog/cnode02/openstack.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }
                        
            vi /etc/logrotate.d/cnode02_syslog
            /var/log/rsyslog/cnode02/syslog.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }
            
            vi /etc/logrotate.d/controller_host_openstack
            /var/log/rsyslog/controller/openstack.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }            
            
            vi /etc/logrotate.d/controller_syslog
            /var/log/rsyslog/controller/syslog.log {
                daily
                missingok
                compress
                delaycompress
                notifempty
            }
            
        - client side            
            1. /etc/rsyslog.d 디렉토리에 client.conf 파일 생성            
            
            # prevent debug from dnsmasq with the daemon.none parameter
            #*.*;auth,authpriv.none,daemon.none,local0.none -/var/log/syslog
            *.*;auth,authpriv.none,daemon.none,local0.* -/var/log/syslog
            
            # Specify a log level of ERROR
            #local0.error @@172.20.1.43:1024
            *.* @@10.0.0.101:5140                

1030

    - trouble shooting: instance가 삭제되지 않을때

        방법1)
            Reset the state of an instance
            
            If an instance remains in an intermediate state, such as deleting, you can use the nova reset-state command to manually reset the state of an instance to an error state. You can then delete the instance. For example:
            
            $ nova reset-state c6bbbf26-b40a-47e7-8d5c-eb17bf65c485
            $ nova delete c6bbbf26-b40a-47e7-8d5c-eb17bf65c485
            You can also use the --active parameter to force the instance back to an active state instead of an error state. For example:
            
            $ nova reset-state --active c6bbbf26-b40a-47e7-8d5c-eb17bf65c485

            root@controller:~/openstack# nova list --all-tenants
            +--------------------------------------+---------------+---------+------------+-------------+---------------------------------------------------------------------------------------+
            | ID                                   | Name          | Status  | Task State | Power State | Networks                                                                              |
            +--------------------------------------+---------------+---------+------------+-------------+---------------------------------------------------------------------------------------+
            | 6185d579-8572-4f34-95b2-5d086e2c8d90 | 001_client    | BUILD   | -          | NOSTATE     |                                                                                       |
            +--------------------------------------+---------------+---------+------------+-------------+---------------------------------------------------------------------------------------+
            
            root@controller:~/openstack# nova reset-state   6185d579-8572-4f34-95b2-5d086e2c8d90
            
            root@controller:~/openstack# nova list --all-tenants
            +--------------------------------------+---------------+---------+------------+-------------+---------------------------------------------------------------------------------------+
            | ID                                   | Name          | Status  | Task State | Power State | Networks                                                                              |
            +--------------------------------------+---------------+---------+------------+-------------+---------------------------------------------------------------------------------------+
            | 6185d579-8572-4f34-95b2-5d086e2c8d90 | 001_client    | ERROR   | -          | NOSTATE     |                                                                                       |
            +--------------------------------------+---------------+---------+------------+-------------+---------------------------------------------------------------------------------------+
            
            root@controller:~/openstack# nova delete  6185d579-8572-4f34-95b2-5d086e2c8d90
            
    - trouble shooting: floating ip 가 동작하지 않을때
        - L3 Agent가 사용하는 router 를 확인한다.
            root@controller:~/openstack/test_senario/senario2# neutron router-list
            +--------------------------------------+--------------------+-----------------------------------------------------------------------------+
            | id                                   | name               | external_gateway_info                                                       |
            +--------------------------------------+--------------------+-----------------------------------------------------------------------------+
            | 14e3e4d4-6a4f-4079-91f3-372b3ff8ec04 | aaa_guest_router   | {"network_id": "d2e6b85d-f429-4b2e-86f4-ee5799a60b92", "enable_snat": true} |
            | 2f9c2459-9c66-49a2-8665-c416d741e483 | 001_guest_router   | {"network_id": "d2e6b85d-f429-4b2e-86f4-ee5799a60b92", "enable_snat": true} |
            | 442b597e-beb1-4224-a485-e8e26015cf0f | global_mgmt_router | {"network_id": "d2e6b85d-f429-4b2e-86f4-ee5799a60b92", "enable_snat": true} |
            | 6fb2c74f-ba51-40b4-8c8a-7fbbb47c39a6 | aaa_guest_router   | {"network_id": "d2e6b85d-f429-4b2e-86f4-ee5799a60b92", "enable_snat": true} |
            | 80569295-fc70-48d1-a2cd-2c2c6902167d | test1_guest_router | {"network_id": "d2e6b85d-f429-4b2e-86f4-ee5799a60b92", "enable_snat": true} |
            | 9724887a-8a24-40c6-8766-cc6793b60588 | bbb_guest_router   | {"network_id": "d2e6b85d-f429-4b2e-86f4-ee5799a60b92", "enable_snat": true} |
            | ab10283a-27c3-412c-a2ab-600702db55f2 | 002_guest_router   | null                                                                        |
            +--------------------------------------+--------------------+-----------------------------------------------------------------------------+

        - ip netns 명령을 이용하여 해당 라우터(global_mgmt_router)가 사용하는 namespace를 파악한다.
            root@controller:~/openstack/test_senario/senario2# ip netns 
            qrouter-ab10283a-27c3-412c-a2ab-600702db55f2
            qdhcp-a1c6663c-05a9-454a-a941-fa322c001a93
            qdhcp-14df249e-bb3a-4667-b7ca-1e4252fd7b78
            qrouter-2f9c2459-9c66-49a2-8665-c416d741e483
            qdhcp-c54023b3-4e6e-402a-b828-967e9b3e49fa
            qdhcp-aa43c21a-348c-4388-ba4a-35b818010346
            qdhcp-8ab1018e-517c-4fa2-86c9-531a24f0480c
            qdhcp-532d8a87-3d0c-44f2-9ea0-01ed132729fc
            qdhcp-07405330-ca1c-4fba-ad9d-ae3e992d8164
            qrouter-9724887a-8a24-40c6-8766-cc6793b60588
            qrouter-14e3e4d4-6a4f-4079-91f3-372b3ff8ec04
        
        - 해당 라우터에 설정된 iptable을 확인한다.
            
            
    
1029
    - LJG: 성능 측정 및 수집방법
       - global_mgmt_net에 연결된 하나의 vm에서 
         모든 VM에게 원격으로 명령을 내리고 수집하는 구조가 더 효율적임
         
        - client : iperf 만 설치
        - server : nohup /usr/bin/iperf -s > /dev/null &2>1 & 
        - mgmt_vm: 
            1. 모든 client vm에 원격으로 iperf -c 명령으로 성능테스트 시작
                /usr/bin/iperf -c ${_server_ip} -i 5 -t 50 -P 5 > iperf.txt &
            2. 모든 client vm에 원격으로 iperf 성능데이터 수집
                cat ~/iperf.txt | grep SUM
                
              

1026
    - neutron port create 권한 부여 
    neutron port-create --fixed-ip subnet_id=34e28ba6-31d3-4253-924e-ef3a0150da9f,ip_address=10.10.10.222 07405330-ca1c-4fba-ad9d-ae3e992d8164
    {"NeutronError": {"message": "Policy doesn't allow create_port to be performed.", "type": "PolicyNotAuthorized", "detail": ""}}

    - /etc/neutron/policy.json 파일에 다음항목 수정
    원본  :"create_port:fixed_ips": "rule:admin_or_network_owner",
    수정본:"create_port:fixed_ips": "rule:admin_or_network_owner or role:member",

1024
    - LJG: 지금까지 헛고생한 것을 총정리해보면...
        메뉴얼에는 config를 수작업으로 작성하는데 
        나는 자동화를 좋아하다 보니
        하나의 라이브러리고 여러개의 배포작업을 수행하고 싶었다.
        그러나, 이게 완전히 독이 되었는데
        
        neutron.conf같은 경우 3개의 파일에서 서로 수정하다보니 통일이 안되어 뒤죽박죽 되었고
        이에 따라 에러가 빈번히 발생했다.
        
        오늘 집에서 one_node에 설치하는 경우
        neutron.conf/nova.conf등 여러군데서 수정하는 파일들은 모두 한곳에서 통일되게 수정하도록 
        작업해야 겠다.
        
        config는 종합적인 관점에서 한곳에서만 고쳐야 한다. 
        정말 많은 개고생이 있었다.
        
         

1023
	- horizon의 에러를 쉽게 브라우저에서 확인하고 싶으면 아래 설정 추가
		sed -i "s/DEBUG = \"False\"/DEBUG = \"True\"/g" /etc/openstack-dashboard/local_settings.py		
		sed -i "s/TEMPLATE_DEBUG = \"False\"/TEMPLATE_DEBUG = \"True\"/g" /etc/openstack-dashboard/local_settings.py
		
	- ubuntu network nic setting
		/etc/network/interface 파일을 만들때 #을 넣으면 에러가 난다.
		왜인지 이유는 모르겠다. 아래와 같이 주석을 넣으면 에러가 발생
			
		# mgmt network  
		auto em1
		iface em1 inet static
		    address 10.0.0.101
		    netmask 255.255.255.0
		    
	- LJG: nova-api(8774)와 nova-api-metadata(8775)를 따로 생성하면 nova-api는 인스턴스만 생성하고
	  8774번 포트를 listen하지 못한다.
	  정말 openstack은 미스테리 투성이다.

	- LJG: nova-manage service list 를 실행하면 웬 DEBUG 메시지가 이렇게 출력될까?
 
	2014-10-24 03:03:51.795 18581 DEBUG nova.servicegroup.api [-] ServiceGroup driver defined as an instance of db __new__ /usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:65
	2014-10-24 03:03:51.878 18581 DEBUG stevedore.extension [-] found extension EntryPoint.parse('file = nova.image.download.file') _load_plugins /usr/lib/python2.7/dist-packages/stevedore/extension.py:156
	2014-10-24 03:03:51.891 18581 DEBUG stevedore.extension [-] found extension EntryPoint.parse('file = nova.image.download.file') _load_plugins /usr/lib/python2.7/dist-packages/stevedore/extension.py:156
	2014-10-24 03:03:52.446 18581 DEBUG nova.servicegroup.api [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] Check if the given member [{'binary': u'nova-scheduler', 'availability_zone': 'internal', 'deleted': 0L, 'created_at': datetime.datetime(2014, 10, 23, 15, 39, 10), 'updated_at': datetime.datetime(2014, 10, 23, 18, 3, 47), 'report_count': 757L, 'topic': u'scheduler', 'host': u'controller', 'disabled': False, 'deleted_at': None, 'disabled_reason': None, 'id': 1L}] is part of the ServiceGroup, is up service_is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:116
	2014-10-24 03:03:52.447 18581 DEBUG nova.servicegroup.drivers.db [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] DB_Driver.is_up last_heartbeat = 2014-10-23 18:03:47 elapsed = 5.447715 is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py:71
	2014-10-24 03:03:52.448 18581 DEBUG nova.servicegroup.api [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] Check if the given member [{'binary': u'nova-consoleauth', 'availability_zone': 'internal', 'deleted': 0L, 'created_at': datetime.datetime(2014, 10, 23, 15, 39, 10), 'updated_at': datetime.datetime(2014, 10, 23, 18, 3, 47), 'report_count': 760L, 'topic': u'consoleauth', 'host': u'controller', 'disabled': False, 'deleted_at': None, 'disabled_reason': None, 'id': 2L}] is part of the ServiceGroup, is up service_is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:116
	2014-10-24 03:03:52.448 18581 DEBUG nova.servicegroup.drivers.db [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] DB_Driver.is_up last_heartbeat = 2014-10-23 18:03:47 elapsed = 5.448725 is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py:71
	2014-10-24 03:03:52.449 18581 DEBUG nova.servicegroup.api [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] Check if the given member [{'binary': u'nova-conductor', 'availability_zone': 'internal', 'deleted': 0L, 'created_at': datetime.datetime(2014, 10, 23, 15, 39, 11), 'updated_at': datetime.datetime(2014, 10, 23, 18, 3, 47), 'report_count': 757L, 'topic': u'conductor', 'host': u'controller', 'disabled': False, 'deleted_at': None, 'disabled_reason': None, 'id': 3L}] is part of the ServiceGroup, is up service_is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:116
	2014-10-24 03:03:52.449 18581 DEBUG nova.servicegroup.drivers.db [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] DB_Driver.is_up last_heartbeat = 2014-10-23 18:03:47 elapsed = 5.449656 is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py:71
	2014-10-24 03:03:52.450 18581 DEBUG nova.servicegroup.api [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] Check if the given member [{'binary': u'nova-cert', 'availability_zone': 'internal', 'deleted': 0L, 'created_at': datetime.datetime(2014, 10, 23, 15, 39, 11), 'updated_at': datetime.datetime(2014, 10, 23, 18, 3, 46), 'report_count': 758L, 'topic': u'cert', 'host': u'controller', 'disabled': False, 'deleted_at': None, 'disabled_reason': None, 'id': 5L}] is part of the ServiceGroup, is up service_is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:116
	2014-10-24 03:03:52.450 18581 DEBUG nova.servicegroup.drivers.db [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] DB_Driver.is_up last_heartbeat = 2014-10-23 18:03:46 elapsed = 6.450555 is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py:71
	2014-10-24 03:03:52.451 18581 DEBUG nova.servicegroup.api [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] Check if the given member [{'binary': u'nova-compute', 'availability_zone': 'nova', 'deleted': 0L, 'created_at': datetime.datetime(2014, 10, 23, 17, 14, 20), 'updated_at': datetime.datetime(2014, 10, 23, 18, 3, 48), 'report_count': 275L, 'topic': u'compute', 'host': u'controller', 'disabled': False, 'deleted_at': None, 'disabled_reason': u'None', 'id': 6L}] is part of the ServiceGroup, is up service_is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:116
	2014-10-24 03:03:52.451 18581 DEBUG nova.servicegroup.drivers.db [req-3e91bb0f-5aa8-4f37-8809-4882912b1432 None None] DB_Driver.is_up last_heartbeat = 2014-10-23 18:03:48 elapsed = 4.451478 is_up /usr/lib/python2.7/dist-packages/nova/servicegroup/drivers/db.py:71

	Binary           Host                                 Zone             Status     State Updated_At
	nova-scheduler   controller                           internal         enabled    :-)   2014-10-23 18:03:47
	nova-consoleauth controller                           internal         enabled    :-)   2014-10-23 18:03:47
	nova-conductor   controller                           internal         enabled    :-)   2014-10-23 18:03:47
	nova-cert        controller                           internal         enabled    :-)   2014-10-23 18:03:46
	nova-compute     controller                           nova             enabled    :-)   2014-10-23 18:03:48
	
	/usr/lib/python2.7/dist-packages/nova/servicegroup/api.py:115 줄의 debug 출력을 막아야 한다.
	
	# LOG.debug(msg, {'member_id': member_id, 'group_id': group_id, 'service': service})
	    
1022
	LJG: openstack log 구조
	
		- 처음 실행시 로그 
			/var/log/upstart 디렉토리에 모든 콤포넌트를 파일로 쌓는다.
			따라서 초창기 실행이 안될때는 이 디렉토리의 해당 콤포넌트 로그를 모니터해야 한다.
			
		- 동작시 로그
			/var/log/ 밑에 개별 콤포넌트별로 디렉토리를 만들어 쌓는다.
				
			/var/log/nova
					 cinder
					 glance
					 neutron 등등			 
					 
	- option 설정 에러
		이게 잘못되면 다음과 같은 에러가 발생				
			File "/usr/lib/python2.7/dist-packages/oslo/config/cfg.py", line 500, in _is_opt_registered
	    	raise DuplicateOptError(opt.name)
			oslo.config.cfg.DuplicateOptError: duplicate option: pybasedir
			
		소스를 찾아보면 다음과 같이 설명
		즉, 중복해서 옵션을 설정했다. 동일항목에 다른값으로 중복설정하면 에러가 난다.
		
			"""Check whether an opt with the same name is already registered.

		    The same opt may be registered multiple times, with only the first
		    registration having any effect. However, it is an error to attempt
		    to register a different opt with the same name.
		
		    :param opts: the set of opts already registered
		    :param opt: the opt to be registered
		    :returns: True if the opt was previously registered, False otherwise
		    :raises: DuplicateOptError if a naming conflict is detected
		    """					 
1021	

    - How to see log to find a boot problem    
        /var/log/boot.log  ---  System boot log
        /var/log/dmesg     ---  print or control the kernel ring buffer

        # dmesg | less
        

1020
	pexpect를 이용한 모니터링
	cmd = "netstat -nao|egrep '\(LISTEN\|EST\)'|egrep '\(22\|3306\)'"
	cmd = "ls -al /var/log/cdp_agent"
	cmd = "crontab -l"
	cmd = "rm -rf /var/log/cdp_agent"
	cmd = 'field "is_control_domain" = "false" and field "power_state" = "Running" and field "is_a_template" = "false" and field "is_a_snapshot" = "false" and field "resident_on" = "%s"'
    cmd = "xe host-param-get uuid=`xe host-list name-label=\`hostname\` params=uuid --minimal` param-name=software-version param-key=product_version"
    cmd = """xe host-param-get uuid=`xe host-list name-label=%s params=uuid --minimal` param-name=software-version param-key=product_version""" % (hostname)
    cmd = "(/usr/bin/crontab -l; echo '*/30 * * * * /usr/bin/python /var/log/cdp_agent/DiskUsageAgent.py >/dev/null 2>&1') | /usr/bin/crontab -"
	cmd = '/usr/bin/crontab -u root -l'
	cmd  = 'ssh -p %s %s@%s %s'%(port, id, ip, cmd)
	check_cmd = """netstat -naop | grep 8501 | grep LISTEN | grep haproxy | grep -v grep | wc -l"""
	check_cmd = """netstat -naop | grep 6379 | grep LISTEN | grep redis | grep -v grep | wc -l"""
	haproxy_start_cmd = '/usr/local/sbin/haproxy -f /etc/haproxy/haproxy.conf'
	redis_start_cmd = '/root/redis-2.8.1/src/redis-server'            

	# LJG: pexpect에서 명령어 실행을 성공하려면 ',"와 같은 escape문자에 대한 적절한 처리가 필요하다.
    # 이를 위해서는 아래와 같이 '\\'문자를 escape문자앞에 2번 넣어주어야 한다.
    #    ' -> \\'
    #    " -> \\"
    # 이를 위해 다음과 같은 전처리가 필요 :
    #    cmd = cmd.replace("'", "\\'").replace('"', '\\"')
    # ex)
    #     before: netstat -naop | egrep '(8501|8502|8503|8504|6379)' | grep LISTEN |sort 
    #     after : netstat -naop | egrep \'(8501|8502|8503|8504|6379)\' | grep LISTEN |sort
    
    cmd = """netstat -naop | egrep '(8501|8502|8503|8504|6379)' | grep LISTEN |sort | awk '{if ($4 && $7) print $4" "$7}' """
    log.debug( "before: <%s>" % cmd )
    cmd = cmd.replace("'", "\\'").replace('"', '\\"')   
    log.debug( "CMD: <%s>" % cmd )
    result = self.rssh.doRemoteCommand ('cluster_1', cmd)

1019
	rabbitmq management enable
	
	1. web console
		cd /usr/lib/rabbitmq/bin
		rabbitmq-plugins enable rabbitmq_management
		service rabbitmq-server restart
		
		http://211.224.204.156:55672/
		
		id/pw: guest/guest
	
	2. rabbitmqctl
	
		Commands:
	    stop [<pid_file>]
	    stop_app
	    start_app
	    wait <pid_file>
	    reset
	    force_reset
	    rotate_logs <suffix>
	
	    cluster <clusternode> ...
	    force_cluster <clusternode> ...
	    cluster_status
	
	    add_user <username> <password>
	    delete_user <username>
	    change_password <username> <newpassword>
	    clear_password <username>
	    set_user_tags <username> <tag> ...
	    list_users
	
	    add_vhost <vhostpath>
	    delete_vhost <vhostpath>
	    list_vhosts [<vhostinfoitem> ...]
	    set_permissions [-p <vhostpath>] <user> <conf> <write> <read>
	    clear_permissions [-p <vhostpath>] <username>
	    list_permissions [-p <vhostpath>]
	    list_user_permissions [-p <vhostpath>] <username>
	
	    list_queues [-p <vhostpath>] [<queueinfoitem> ...]
	    list_exchanges [-p <vhostpath>] [<exchangeinfoitem> ...]
	    list_bindings [-p <vhostpath>] [<bindinginfoitem> ...]
	    list_connections [<connectioninfoitem> ...]
	    list_channels [<channelinfoitem> ...]
	    list_consumers [-p <vhostpath>]
	    status
	    environment
	    report
	    eval <expr>
	
	    close_connection <connectionpid> <explanation>
	    trace_on [-p <vhost>]
	    trace_off [-p <vhost>]		
	

1014
	- LJG: VM IP 할당(매우 중요)		

	- 고려사항: openstack 보안정책
		오픈스택은 기본적으로 VM에 IP를 할당하고 이를 해당 cnode의 iptables에 반영시킨다.
		즉, VM에 192.168.0.9를 할당했으면 이와 관련해서 cnode에 다음과 같은 iptables 를 생성한다.
		
		Chain neutron-openvswi-s7f9f4a23-e (1 references)
        num   pkts bytes target     prot opt in out  source       destination         
        1        0     0 RETURN     all  --  *   *   192.168.0.9  0.0.0.0/0   MAC FA:16:3E:D2:52:84
        2        0     0 DROP       all  --  *   *   0.0.0.0/0    0.0.0.0/0 
        
        VM에서 밖으로 패킷을 전송할 때 소스IP가 192.168.0.9가 아니면,
        보안(spoofing)에 어긋난다고 생각하고 해당 cnode의 iptables에서 그 패킷을 drop 시킨다.
        따라서 VM은 무조건 openstack에서 할당한 IP를 src IP로 사용하여 전송해야 한다.
        따라서, 우리가 임의로 특정 VM의 IP를 변경하는 경우 해당 VM은 전송이 안되므로
        어떤 시나리오에 따라 VM에 우리가 원하는 IP를 설정하는 것이 필요할 때에는
        해당 IP로 port를 미리 만들고 이를 이용하여 nova boot 시에 fixed ip를 할당하여 생성해야 한다.
        
        ex) nova boot $vm --flavor $flavor --image $image --nic port-id=$port_id --security-groups default   
        
        그리고 VM내부에서 이 IP를 사용하는 경우에는 userdata를 이용해서 통해 정확하게 전달해야 한다.
        이를 위해서는 템플릿을 이용하여 userdata를 동적으로 만들어야 한다. 
        
        이 문제는 우리가 작성하려는 Firewall VM에도 심각한 문제가 되는데, 
        기본적으로 방화벽이 A("192.168.0.3")에서 받은 패킷을 B("10.10.10.3")에 전송할 때 
        cnode에서 패킷 소스가 "192.168.0.9"가 아니기 때문에 drop 시킨다.
	
			
	
	
	- userdata 사용시 개선사항
	
	1. 근본적으로 vm에 적용되는 아래와 같은 명령은 리부팅이 되어도 실행되게 해야함.
		echo "
		# --------------------------------------------------- 
		# 3. green/orange nic 활성화
		# ---------------------------------------------------
		"
		# green nic 
		sudo ifconfig eth1 up
		
		# orange nic 
		sudo ifconfig eth2 up
		    
		echo "
		# --------------------------------------------------- 
		# 4. green/orange interface 를 bridge 로 연결
		# ---------------------------------------------------
		"
		sudo brctl addbr br0
		sudo brctl addif br0 eth1
		sudo brctl addif br0 eth2
		    
		echo "
		# --------------------------------------------------- 
		# 5. bridge 에 green interface ip 연결
		# ---------------------------------------------------
		"                            
		sudo ifconfig br0 192.168.0.1/24 
	
	2. bootstrap 파일을 템플릿을 적용하여 nova boot를 실행하기 적에 bootstrap파일을 만들고 
	   이를 nova boot에 파라미터로 넘겨야 함.
	   예) ip, hostname 등	
	
	3. sudo: unable to resolve host host_name 에러발생
	   /etc/hosts의 host명과 /etc/hostname의 host명이 틀릴때 나오는 메세지
	   /etc/hostname에 기입되 있는 host명을 /etc/hosts에 추가해 준다.
	   근본적으로 이렇게 만들려면 nova boot 명령이전에 
	   템플릿 기반으로 bootstrap.sh 파일을 만든 다음 이를 nova boot에 넘겨야 한다는 야그


1013(미해결)
    - 수작업으로 iptables에 insert를 하면 openstack에 의해 지워지는 문제
     
    root@havana:~/openstack/havana/bin# iptables -L neutron-openvswi-saef0c8be-3 -n
    Chain neutron-openvswi-saef0c8be-3 (1 references)
    target     prot opt source               destination         
    RETURN     all  --  192.168.0.225        0.0.0.0/0            MAC FA:16:3E:2F:C5:1E
    DROP       all  --  0.0.0.0/0            0.0.0.0/0           
    
    root@havana:~/openstack/havana/bin# iptables -I neutron-openvswi-saef0c8be-3 2 -s 0.0.0.0/0 -d 0.0.0.0/0 -p all -j RETURN
    
    root@havana:~/openstack/havana/bin# iptables -L neutron-openvswi-saef0c8be-3 -n
    Chain neutron-openvswi-saef0c8be-3 (1 references)
    target     prot opt source               destination         
    RETURN     all  --  192.168.0.225        0.0.0.0/0            MAC FA:16:3E:2F:C5:1E
    RETURN     all  --  0.0.0.0/0            0.0.0.0/0
    DROP       all  --  0.0.0.0/0            0.0.0.0/0        
    
    # LJG: iptables를 아래와 같이 리눅스쉘에서 직접 수행하면 이전 수행내역이 지워지지 않는다.
    #      그러나, openstack에서 vm을 생성한 이후에 이전 VM들의 iptables를 조사하면 지워진다.
    #      이는 openstack이 내부적으로 iptables 정보를 관리하고 있다는 얘기이므로 
    #      어디서 이를 관리하는지 파악하여 그곳에서 수정해야 한다.
    
    iptables -I neutron-openvswi-se075a028-6 2 -s 0.0.0.0/0 -d 0.0.0.0/0 -p all -j RETURN
    iptables-save > iptable-rule; iptables-restore < iptable-rule
    iptables -L neutron-openvswi-se075a028-6 -n
    
    iptables -I neutron-openvswi-s380a5a04-5 2 -s 0.0.0.0/0 -d 0.0.0.0/0 -p all -j RETURN
    iptables-save > iptable-rule; iptables-restore < iptable-rule
    iptables -L neutron-openvswi-s380a5a04-5 -n
    
    iptables -I neutron-openvswi-se50d688a-0 2 -s 0.0.0.0/0 -d 0.0.0.0/0 -p all -j RETURN
    iptables-save > iptable-rule; iptables-restore < iptable-rule
    iptables -L neutron-openvswi-se50d688a-0 -n
    
1012
	- mysql Warning: Row 3 was cut by GROUP_CONCAT() 에러발생시 해결방법
 	  디폴트가 1024바이트이므로 group_concat()함수 출력결과 사이즈를 크게해야 한다.
   
   	이를 확실하게 해결하려면 /etc/mysql/my.cnf 파일에서
   	[mysqld] group header아래에 다음라인을 입력해야 한다.

	[mysqld]
	group_concat_max_len=15360


	아니면 매번 접속할 때마다 다음 명령을 사용한다.
	
	$ mysql> set @@group_concat_max_len = 50240;
		
	


1011
- ubuntu firewall test
	Ubuntu로 UTM 에뮬레이션 준비
	0. Ubuntu-UTM VM 생성
	
	0) Havana(211.224.204.147) 에 접속하여 /root/admin/openstack/nova 폴더로 이동
	
	# cd /root/admin/openstack/nova
	
	1) nova-boot.sh를 수정하여 VM에 할당할 NIC 정보 설정
	
	181 let "nw_bit=2#1011"
	182 make_customer_vm \
	183     ubuntu_utm ubuntu-12.04 seocho.seoul.zo.kt havana \
	184     $nw_bit $RED_PUBLIC_NET $GREEN_VLAN1_NET $ORANGE_VLAN1_NET $TEST_NET $PUB_NET
	
	* nw_bit에 설정되는 각 비트는 각각 아래와 같이 연결됨
	- 0001 = GREEN Net
	- 0010 = ORANGE Net
	- 0100 = RED Net
	- 1000 = Guest Net
	
	* VM에 NIC을 할당하는 순서는 1) Guest Net, 2) GREEN Net, 3) ORANGE Net, 4) RED Net 임
	
	1. 사전준비
	
	Havana(211.224.204.147) 에서 Ubuntu_UTM의 GREEN/ORANGE tap/qvb 인터페이스의 이름과 linux bridge 정보를 list-vm.sh를 실행하여 확인
	
	ubuntu-utm
	        DEV_IP= 10.10.10.7     DEV_TAP= tapc39d9b26-2e BR_Q= qbrc39d9b26-2e DEV_QVB= qvbc39d9b26-2e DEV_QVO= qvoc39d9b26-2e
	        DEV_IP= 192.168.0.5    DEV_TAP= tapa0ed3174-94 BR_Q= qbra0ed3174-94 DEV_QVB= qvba0ed3174-94 DEV_QVO= qvoa0ed3174-94
	        DEV_IP= 192.168.0.229  DEV_TAP= tapc904df21-71 BR_Q= qbrc904df21-71 DEV_QVB= qvbc904df21-71 DEV_QVO= qvoc904df21-71
	
	green vm의 ip 정보 확인
	forbiz_green_net: 192.168.0.3
	forbiz_guest_net: 10.10.10.2
	
	orange vm의 ip 정보 확인
	forbiz_orange_net: 192.168.0.227
	forbiz_guest_net: 10.10.10.5
	
	2. 루프 방지를 위해 ubuntu-utm의 qbr들에 대해 hairpin off를 수행
	
	Green 인터페이스쪽
	# brctl hairpin qbra0ed3174-94 tapa0ed3174-94 off
	# brctl hairpin qbra0ed3174-94 qvba0ed3174-94 off
	
	Orange 인터페이스쪽
	# brctl hairpin qbrc904df21-71 tapc904df21-71  off
	# brctl hairpin qbrc904df21-71 qvbc904df21-71  off
	
	3. Ubuntu-UTM 설정
	
	0) Havana(211.224.204.147) 에 접속하여 /root/admin/openstack/netns 폴더로 이동하여 netns.sh 명령을 이용하여 Ubuntu-UTM VM에 접속
	
	# cd /root/admin/openstack/netns/
	# ./netns.sh ssh 10.10.10.7
	
	1) brctl 명령을 이용하기 위해 bridge-utils 설치
	
	# apt-get -y update
	# apt-get install -y bridge-utils
	
	2) ip_forward를 활성화
	
	# cat /proc/sys/net/ipv4/ip_forward
	# echo 1 > /proc/sys/net/ipv4/ip_forward
	
	3) GREEN (eth1) 과 ORANGE (eth2) 인터페이스를 up 
	
	#  ifconfig eth1 up
	#  ifconfig eth2 up
	
	4) brctl 명령으로 br0 생성 및 GREEN/ORANGE 인터페이스를 br0에 연결
	
	# brctl addbr br0
	# brctl addif br0 eth1
	# brctl addif br0 eth2
	
	5) br0에 GREEN 인터페이스에 할당된 IP인 192.168.0.5를 할당
	
	#  ifconfig br0 192.168.0.5/24
	
	4. Havana의 iptables룰 수정
	
	1) 211.224.204.132에 접속하여 /root/admin/openstack/test/network 폴더로 이동
	
	# cd /root/admin/openstack/test/network/
	#  vi cfg/test-node.cfg
	
	2) test-node.cfg 내용을 수정하여 ubuntu-utm의 GREEN/ORANGE 인터페이스 정보를 입력
	- 주의할 점은 tapa0ed3174-94 중 tap이후 10글자인 a0ed3174-9만 입력해야 함
	
	TAP_GREEN_ID="a0ed3174-9"
	NET_GREEN="0.0.0.0/0"
	TAP_ORANGE_ID="c904df21-7"
	NET_ORANGE="0.0.0.0/0"
	
	3)  add-cnode-iptables-rules.sh 를 실행하여 Havana iptables rule에 0.0.0.0/0에 대한 라인을 삽입
	
	# ./add-cnode-iptables-rules.sh
	
	5. Green VM에 접속하여 연결 테스트
	
	0) Havana(211.224.204.147) 에 접속하여 /root/admin/openstack/netns 폴더로 이동하여 netns.sh 명령을 이용하여 Green VM에 접속
	
	# cd /root/admin/openstack/netns/
	# ./netns.sh ssh 10.10.10.2
	
	1) Ubuntu-UTM의 br0에 ping 
	
	# ping 192.168.0.5
	
	2) Orange node의 br0에 ping
	
	# ping 192.168.0.227



1010
- 분명히 스크립트(특히 파이썬)파일도 있고 패스도 잡혀있는데 실행이 안되는 경우
  이건 dos 파일포맷이 유닉스에서 실행되서 발생되는 현상    
  dos2unix 명령사용해서 유닉스 파일 포맷으로 변경해야 함.
  예) 
    root@havana:~/openstack/ref/old/utility/linux/linux_metrics-0.1.0# ./example.py
    : No such file or directory
    
    root@havana:~/openstack/ref/old/utility/linux/linux_metrics-0.1.0# dos2unix example.py
    dos2unix: converting file example.py to Unix format ...

    root@havana:~/openstack/ref/old/utility/linux/linux_metrics-0.1.0# ./example.py
    procs running: 1
    cpu utilization: 0.62%
    disk busy: 0.0%
    disk reads: 457542
    disk writes: 4772547
    mem used: 33596858368
    mem total: 135165214720
    net bits received: 4086669152
    net bits sent: 3076171496

1009
- LJG: 일반 user 계정으로는 guest network를 제외한 hybrid network를 생성할 수 없다.
       그래서 admin 계정으로 network를 만들고 --tenant-id를 통해 특정 계정의 소유로 제공한다.
       이를 수정하려면 /etc/neutron/policy.json 파일을 변경해야 하나 권장할 사항은 아니다.

- LJG: multinic VM의 경우 guest network NIC을 맨처음에 할당해야 dhcp를 통해 ip를 할당받을 수 있다.
      왜 그런지는 모르겠는데 multinic을 할당해도 /etc/network/interface 디렉토리에 eth0번 정보만
      입력되 있어 우리처럼 dhcp를 사용안한 subnet을 사용하는 nic을 맨처음에 할당하면 
      IP 할당이 안되서 외부와 통신이 안된다. 
      그러면 nova boot를 이용하여 VM을 생성할 때 userdata를 통해 외부에
      접속하여 프로그램을 설치할 수 없다.
  
    ex) nova boot $vm
        --flavor 3
        --image $image
        --key-name $GUEST_KEY
        --nic net-id=$_guest_net_id
        --nic net-id=$_green_net_id
        --nic net-id=$_orange_net_id
        --availability-zone ${zone}:${host}
        --security-groups default



1007
- LJG: openstack은 기본적으로 utcnow()함수로 UTC를 사용하여 KST보다 9시간 느리다.
    이것을 모두 KST(Asia/Seoul)로 바꿔줘야 한다.
    그러나 모두 무식하게 소스에 박혀있어 변경이 어렵다.
    근본적으로 9시간 느리다고 인식하고 있어야 한다. 오픈스택이 timezone을 지원할 때 까지
    정말 왕짜증... 아니면, 일일이 시간변환 함수를 쓰란다... 미쳤나??  
    그나마, Hoziron을 TimeZone support 기능이 있어 포탈의 설정에서 바꾸거나 아래처럼 local_settings.py에서 변경가능
    처음에는 데이터베이스 설정이 잘못된 줄 알고 한참 해맸다.

    here is a line in /etc/openstack-dashboard/local_settings.py called TIME_ZONE

    # The timezone of the server. This should correspond with the timezone
    # of your entire OpenStack installation, and hopefully be in UTC.
    TIME_ZONE = "Asia/Seoul"

    Change it, restart apache and memcached, that should do the trick.


1006
- LJG(매우 중요!!!): 일반계정에 권한을 부여하는 방법
  예를 들면 admin 권한이 없는 일반계정(ex: forbiz)에 availability zone 설정권한을 주려면 
  /etc/nova/policy.json 파일을 편집해야 한다. 파일만 편집해도 자동으로 적용된다.
  
  default
  "compute:create:forced_host": "is_admin:True",
  
  수정본
  "compute:create:forced_host": "is_admin:True or role:member",
  
    # 
    http://prosuncsedu.wordpress.com/2013/09/28/policy-administration-for-openstack-nova/
    
    The default installation of openstack devstack comes with ‘/etc/nova/policy.json’ which is the policy administration point for openstack nova. Following is an excerpt from this file
        
    "compute_extension:admin_actions:pause": "rule:admin_or_owner"
    
    So, what it means if you want to pause (probably, pausing a vm), you have to be the owner or the admin. If we want to remove this checking, we may simply, erase the rule, making everyone able to pause the vm by
    
    
    "compute_extension:admin_actions:pause": ""
    
    Now, if you want to modify the policy such that, admin or owner or someone with role: trainer can do that very action, change it as follows:
    
    
    "compute_extension:admin_actions:pause": "rule:admin_or_owner or role:trainer"
    
    Now, If you want to insert a new policy, add a new line with the policy name. For example, I have added a policy for nova by adding this line where the policy name is “compute:detailall”:
    
    "compute:detailall":"role:admin or role:tadmin".
    
    Worth to note that, this policy is only enforced in places where in the code I have used and enforced this policy. For example,
    I have added this line in place where I wanted to enforce this policy.
    
    ctxt = req.environ['nova.context']
    policy.enforce(ctxt,'compute:detailall',{'getall':None})
    
    Anyway, there are whole lot of other issues about policy administration which we may visit later.


- LJG: 개고생: openstack은 실제에러와 동떨어진 엉뚱한 에러메시지를 보여주어 디버깅을 상당히 어렵게 하는 경우가 많다.
  예) "Maximum number of ports exceeded"라는 에러가 발생하기에 quota에 할당한 port가 적어서 문젠가 보다 생각해서
      port quota를 500으로 늘려줘도 동일에러가 발생했슴.
      원인은 red_shared_public_net 에 할당한 ip_pool이 모두 사용중이라 VM이 red_shared_public_net 용 nic을 요청하면
      이에 할당할 IP가 없어서 발생한 문제이다.

0927

    # --------------------------------------------------------------------------------------------------------------
    # LJG: 오픈스택 설치와 사용을 위해서는 2개의 인증파일이 필요
    #  format: 사용자명.인증방법.인증파일확장자(cred)
    #       ex) admin.token.cred -> admin 사용자가 token을 사용하여 keystone에 인증하기 위해 사용하는 파일
    #                               이는 openstack 초장기에만 사용함
    #       ex) admin.user.cred -> admin 사용자가 username/password를 사용하여 keystone에 인증하기 위해 사용하는 파일
    #                              이는 설치가 끝난후 개별사용자별로 따로 파일을 만들어서 제공해야 한다.
    #           -> jingoo.user.cred, namgon.user.cred, forbiz.user.cred
    #
    # 1. 설치단계 keystone 사용을 위한 인증파일
    #    admin.token.cred
    #    1-1. /etc/keystone/keystone.conf 파일에서 admin_token 키의 값에 원하는 값을 설정 
    #       admin_token=ohhberry3333 
    #    1-2. ~/admin.token.cred 파일을 생성하고 아래 환경변수 설정     
    #       export OS_SERVICE_TOKEN=ohhberry3333    # 이 값은 1-1에 설정된 값과 동일해야 함.
    #       export OS_SERVICE_ENDPOINT=http://10.0.0.101:35357/v2.0  # mgmt ip 설정
    #
    # 2. 개별 사용자 생성후 api 사용을 위한 인증파일, 개별 사용자마다 하나씩 사용
    #    admin.user.cred, forbiz.user.cred
    #
    #       export OS_AUTH_URL=http://10.0.0.101:5000/v2.0
    #       export OS_TENANT_NAME=admin
    #       export OS_USERNAME=admin
    #       export OS_PASSWORD=ohhberry3333
    # --------------------------------------------------------------------------------------------------------------

    echo "
    # --------------------------------------------------------------------------
    # 처음설치할 때 사용할 admin.token.cred 파일 생성
    # token을 사용하여 bypassing the password requirement
    # keystone을 설치해서 계정을 생성한 후에는 unset 해야함
    # --------------------------------------------------------------------------
    "

    cat > ~/admin.token.cred <<EOF
    export OS_SERVICE_TOKEN=ohhberry3333
    export OS_SERVICE_ENDPOINT=http://${KEYSTONE_ENDPOINT}:35357/v2.0/
    EOF

    echo "
    # --------------------------------------------------------------------------
    # 사용자들이 사용하게 될 admin.user.cred 파일 생성    
    # keystone을 설치해서 계정을 생성한 후에는 admin.token.cred을 unset 하고
    # admin.user.cred을 source해서 사용해야 함.
    #
    # 이 둘을 함께 사용하지 못함, 
    # 즉 인증을 위해서 token 이나 user/pass 방식중 하나를 사용해야 하고
    # 보통 후자를 기본으로 함. 
    # --------------------------------------------------------------------------
    "    

    cat > ~/admin.user.cred <<EOF
    export OS_TENANT_NAME=admin
    export OS_USERNAME=admin
    export OS_PASSWORD=ohhberry3333
    export OS_AUTH_URL=http://${KEYSTONE_ENDPOINT}:5000/v2.0/
    export OS_NO_CACHE=1
    EOF


0926
    문제점 : apt-get -y install glance    
    E: There are problems and -y was used without --force-yes
    
    해결책: --force-yes를 넣어주어야 함.
    apt-get -y --force-yes install glance    
0925
    문제점:
    WARNING: Bypassing authentication using a token & endpoint (authentication credentials are being ignored).
    
    해결책:    
    This warning message indicates Keystone is using admin_token based authentication instead user/passwd one. You can use any of these two methods (but not both at same time):
    
    1) admin_token based method (for admin tasks)
       맨처음에 keystone 설치하는 과정에서 사용자를 만들지 않았을때만 사용        
        export OS_SERVICE_ENDPOINT = http://YOUR_KEYSTONE_SERVER:35357/v2.0
        export OS_SERVICE_TOKEN = your-admin-token
    
    2) user/passwd based method (for user authentication and authorization)
        # keystone으로 사용자를 만들었을 때 사용
        export OS_AUTH_URL = http://YOUR-KEYSTONE-SERVER:5000/v2.0
        export OS_USERNAME = username
        export OS_PASSWORD = user-passwd   
        export OS_TENANT_NAME = user-tenant
        export OS_REGION_NAME = user-region
        
0919
- public floating ip update 방법
  openstack menual에는 subnet update 명령이 존재하나 적용되지 않음.
  따라서 데이터베이스를 직접 조작해야 한다.
  관련 테이블
    - ipallocationpools : subnet에 할당된 IP pool
    - ipallocations : subnet에서 VM에 이미 할당된 ip
    - ipavailabilityranges : subnet에서 VM에 할당 가능한 ip range들
        -> 만약 전체 대역의 중간 IP들이 할당된 경우 range들이 여러개 존재할 수 있다.
    따라서 IP 대역을 늘려주려면
    ipallocationpools을 늘려주고
    ipavailabilityranges에서 가장 마지막 range의 끝 IP를 늘리고자 하는 ip로 변경해야 한다.
    
    ex) before
        ipallocationpools   : 221.145.180.71 ~ 221.145.180.75
        ipallocations       : 221.145.180.71, 221.145.180.73
        ipavailabilityranges: 221.145.180.72~221.145.180.72, 221.145.180.74~75
        
        after : 221.145.180.71 ~ 221.145.180.85로 변경하는 경우
        ipallocationpools   : 221.145.180.71 ~ 221.145.180.85 <- 요기변경
        ipallocations       : 221.145.180.71, 221.145.180.73
        ipavailabilityranges: 221.145.180.72~221.145.180.72, 221.145.180.74~85 <- 요기변경

- vm error trace query

    USE nova;
    CREATE VIEW vw_vm_trace AS 
        SELECT
            a.display_name AS vm_name,
            b.action       AS ACTION,
            c.event        AS event,
            c.start_time   AS start_time,
            c.finish_time  AS finish_time,
            c.result       AS result,
            c.traceback    AS details
        FROM 
            nova.instances a,
            nova.instance_actions b,
            nova.instance_actions_events c
        WHERE a.uuid = b.instance_uuid
            AND b.id = c.action_id
        
        UNION 
        
        SELECT
            a.display_name  AS vm_name,
            b.host          AS HOST,
            "#instance fault",
            b.created_at    AS created_at,
            b.deleted_at    AS deleted_at,
            b.message       AS message,
            b.details       AS details
        FROM 
            nova.instances a,
            nova.instance_faults b
        WHERE a.uuid = b.instance_uuid
    
        -- ORDER BY 1,4

-VM detail info query

    CREATE vw_inventory AS
    SELECT
      ni.availability_zone   AS vm_zone,
      ni.host                AS vm_host,
      ni.hostname            AS vm_name,
      ni.created_at          AS vm_create_dt,
      (SELECT
            GROUP_CONCAT(CONCAT('host total_vcpu(x5): ',(ncn.vcpus * 5),', host used_vcpu: ',ncn.vcpus_used,', percent_vcpu(%): ',FORMAT(((ncn.vcpus_used / (ncn.vcpus * 5)) * 100),1),', total_mm(MB): ',ncn.memory_mb,', used_mm(MB): ',ncn.memory_mb_used,', percent_mm(%): ',FORMAT(((ncn.memory_mb_used / ncn.memory_mb) * 100),1)) SEPARATOR ' || ')
       FROM nova.compute_nodes ncn
       WHERE (ni.host = ncn.hypervisor_hostname)) AS vm_host_infos,
      kp.name                AS vm_project_name,
      (SELECT
            GROUP_CONCAT(CONCAT(nq.resource,' -> ',nq.hard_limit) SEPARATOR ', ')
       FROM nova.quotas nq
       WHERE ((ni.project_id = nq.project_id)
              AND (nq.resource IN('instances','cores','ram')))) AS vm_project_compute_quotas,
      (SELECT
         GROUP_CONCAT(CONCAT(nqu.resource,' -> ',nqu.in_use) SEPARATOR ', ')
       FROM nova.quota_usages nqu
       WHERE ((ni.project_id = nqu.project_id)
              AND (nqu.resource IN('instances','cores','ram')))) AS vm_project_compute_quota_usage,
      (SELECT
         GROUP_CONCAT(CONCAT(cq.resource,' -> ',cq.hard_limit) SEPARATOR ', ')
       FROM cinder.quotas cq
       WHERE (ni.project_id = cq.project_id)) AS vm_project_volume_quotas,
      (SELECT
         GROUP_CONCAT(CONCAT(cqu.resource,' -> ',cqu.in_use) SEPARATOR ', ')
       FROM cinder.quota_usages cqu
       WHERE (ni.project_id = cqu.project_id)) AS vm_project_volume_quotas_usage,
      (SELECT
         GROUP_CONCAT(qsg.name SEPARATOR ', ')
       FROM neutron.securitygroups qsg
       WHERE (ni.project_id = CONVERT(qsg.tenant_id USING utf8))) AS vm_sequrity_group_info,
      ku.name                AS vm_user_name,
      (SELECT
         GROUP_CONCAT(nkp.fingerprint SEPARATOR ', ')
       FROM nova.key_pairs nkp
       WHERE (ni.user_id = nkp.user_id)) AS vm_user_keypair_fingerprints,
      (SELECT
         GROUP_CONCAT(nkp.public_key SEPARATOR ', ')
       FROM nova.key_pairs nkp
       WHERE (ni.user_id = nkp.user_id)) AS vm_user_keypair_public_key,
      ni.display_description AS vm_desc,
      (CASE ni.power_state WHEN '0' THEN 'inactive' WHEN '1' THEN 'active' ELSE '-' END) AS vm_power_state,
      ni.vm_state            AS vm_state,
      nit.name               AS vm_instance_type,
      ni.vcpus               AS vm_vcpus,
      ni.memory_mb           AS vm_memory(MB),
      (SELECT
         COUNT(0)
       FROM cinder.volumes vol
       WHERE (ISNULL(vol.deleted_at)
              AND (ni.uuid = vol.instance_uuid))) AS vm_vol_count,
      (SELECT
         SUM(vol.size)
       FROM cinder.volumes vol
       WHERE (ISNULL(vol.deleted_at)
              AND (ni.uuid = vol.instance_uuid))) AS vm_vol_size_sum(GB),
      (SELECT
         GROUP_CONCAT(CONCAT('vol_name: ',cv.display_name,', size(GB): ',cv.size,', Loc: ',cv.provider_location) SEPARATOR '||')
       FROM cinder.volumes cv
       WHERE (ISNULL(cv.deleted_at)
              AND (ni.uuid = cv.instance_uuid))) AS vm_vol_infos,
      (SELECT
         GROUP_CONCAT(CONCAT(nism.key,' -> ',nism.value) SEPARATOR ' || ')
       FROM nova.instance_system_metadata nism
       WHERE (ni.uuid = nism.instance_uuid)) AS vm_system_meta_infos,
      niic.network_info      AS network_info,
      (SELECT
         COUNT(0)
       FROM neutron.ports qp
       WHERE (ni.uuid = CONVERT(qp.device_id USING utf8))) AS vm_nic_count,
      (SELECT
         GROUP_CONCAT(qp.mac_address SEPARATOR '-')
       FROM neutron.ports qp
       WHERE (ni.uuid = CONVERT(qp.device_id USING utf8))) AS vm_macs,
      (SELECT
         GROUP_CONCAT(qn.name SEPARATOR '-')
       FROM (neutron.ports qp
          JOIN neutron.networks qn)
       WHERE ((ni.uuid = CONVERT(qp.device_id USING utf8))
              AND (qp.network_id = qn.id))) AS vm_networks,
      (SELECT
         GROUP_CONCAT(qp.device_owner SEPARATOR '-')
       FROM neutron.ports qp
       WHERE (ni.uuid = CONVERT(qp.device_id USING utf8))) AS vm_devices_owners,
      (SELECT
         GROUP_CONCAT(qi.ip_address SEPARATOR ', ')
       FROM (neutron.ports qp
          JOIN neutron.ipallocations qi)
       WHERE ((ni.uuid = CONVERT(qp.device_id USING utf8))
              AND (qp.id = qi.port_id))) AS vm_net_ips,
      (SELECT
         GROUP_CONCAT(qs.cidr SEPARATOR '-')
       FROM ((neutron.ports qp
           JOIN neutron.ipallocations qi)
          JOIN neutron.subnets qs)
       WHERE ((ni.uuid = CONVERT(qp.device_id USING utf8))
              AND (qp.id = qi.port_id)
              AND (qi.subnet_id = qs.id))) AS vm_net_cidrs,
      (SELECT
         GROUP_CONCAT(IFNULL(qs.gateway_ip,'null') SEPARATOR '-')
       FROM ((neutron.ports qp
           JOIN neutron.ipallocations qi)
          JOIN neutron.subnets qs)
       WHERE ((ni.uuid = CONVERT(qp.device_id USING utf8))
              AND (qp.id = qi.port_id)
              AND (qi.subnet_id = qs.id))) AS vm_net_gw_ips,
      ni.hostname            AS vm_hostname,
      ni.launched_at         AS vm_start_dt,
      ni.root_device_name    AS vm_root,
      gi.name                AS image_name,
      (gi.size / 1000000000) AS image_size(GB)
    FROM (((((nova.instances ni
           LEFT JOIN keystone.project kp
             ON ((ni.project_id = kp.id)))
          LEFT JOIN keystone.user ku
            ON ((ni.user_id = ku.id)))
         LEFT JOIN glance.images gi
           ON ((ni.image_ref = CONVERT(gi.id USING utf8))))
        LEFT JOIN nova.instance_info_caches niic
          ON (((ni.uuid = niic.instance_uuid)
               AND ISNULL(niic.deleted_at))))
       LEFT JOIN nova.instance_types nit
         ON ((ni.instance_type_id = nit.id)))
    WHERE ISNULL(ni.deleted_at)
    ORDER BY ni.availability_zone,ni.host,ni.hostname,ni.created_at)

0717
- 오픈스택에서 기본서비스(nova, cinder, keystone등)의 설치가 어떻게 이루어지는가?
     아래설명된 방법으로 apt-get install을 실행할 때 패키지가 
     "/tmp/vagrant-cache/apt or /var/cache/apt/archives" 디렉토리에 내려받고
     그 안에 저장된 파일들이 설치된다. 
     이 때 /etc/init/패키지.conf 파일을 통해 서비스 start/stop/auto-start 방법들이 등록된다 

     캐쉬디렉토리(/tmp/vagrant-cache/apt)에 우분투에 설치된 패키지 리스트를 볼수 있다.
./tmp/vagrant-cache/apt/cinder-api_1%3a2014.1-0ubuntu1.1_all.deb
./tmp/vagrant-cache/apt/cinder-common_1%3a2014.1-0ubuntu1.1_all.deb
./tmp/vagrant-cache/apt/cinder-scheduler_1%3a2014.1-0ubuntu1.1_all.deb
./tmp/vagrant-cache/apt/cinder-volume_1%3a2014.1-0ubuntu1.1_all.deb
./tmp/vagrant-cache/apt/libboost-system1.54.0_1.54.0-4ubuntu3.1_amd64.deb
./tmp/vagrant-cache/apt/libboost-thread1.54.0_1.54.0-4ubuntu3.1_amd64.deb
./tmp/vagrant-cache/apt/libnspr4_2%3a4.10.2-1ubuntu1.1_amd64.deb
./tmp/vagrant-cache/apt/libxml2-utils_2.9.1+dfsg1-3ubuntu4.3_amd64.deb
./tmp/vagrant-cache/apt/linux-headers-3.13.0-30-generic_3.13.0-30.55_amd64.deb
./tmp/vagrant-cache/apt/linux-headers-3.13.0-30_3.13.0-30.55_all.deb
./tmp/vagrant-cache/apt/linux-image-3.13.0-30-generic_3.13.0-30.55_amd64.deb
./tmp/vagrant-cache/apt/linux-libc-dev_3.13.0-30.55_amd64.deb
./tmp/vagrant-cache/apt/neutron-common_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/neutron-dhcp-agent_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/neutron-l3-agent_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/neutron-metadata-agent_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/neutron-plugin-ml2_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/neutron-plugin-openvswitch-agent_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/neutron-server_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/nova-ajax-console-proxy_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-api-metadata_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-api_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-cert_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-common_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-compute-libvirt_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-compute-qemu_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-compute_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-conductor_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-consoleauth_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-doc_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-novncproxy_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-objectstore_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/nova-scheduler_1%3a2014.1-0ubuntu1.2_all.deb
./tmp/vagrant-cache/apt/python-cinder_1%3a2014.1-0ubuntu1.1_all.deb
./tmp/vagrant-cache/apt/python-neutron_1%3a2014.1-0ubuntu1.3_all.deb
./tmp/vagrant-cache/apt/python-nova_1%3a2014.1-0ubuntu1.2_all.deb

우분투 패키지 설치방법: http://verejun.blogspot.kr/2011/05/ubuntu.html

특정패키지에 설치된 파일목록 보기
root@cinder:/tmp/vagrant-cache/apt# dpkg -c cinder-api_1%3a2014.1-0ubuntu1.1_all.deb
drwxr-xr-x root/root         0 2014-06-09 15:01 ./
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/bin/
-rwxr-xr-x root/root      1868 2014-06-09 15:01 ./usr/bin/cinder-api -> 설치될 메인 프로그램
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/share/
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/share/doc/
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/share/doc/cinder-api/
-rw-r--r-- root/root      1050 2014-04-17 10:24 ./usr/share/doc/cinder-api/copyright
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/share/man/
drwxr-xr-x root/root         0 2014-06-09 15:01 ./usr/share/man/man8/
-rw-r--r-- root/root       210 2014-06-09 15:01 ./usr/share/man/man8/cinder-api.8.gz
drwxr-xr-x root/root         0 2014-06-09 15:01 ./etc/
drwxr-xr-x root/root         0 2014-06-09 15:01 ./etc/logrotate.d/
-rw-r--r-- root/root       105 2014-06-09 15:01 ./etc/logrotate.d/cinder-api -> cinder-api.log 파일 관리 설정
drwxr-xr-x root/root         0 2014-06-09 15:01 ./etc/init/
-rw-r--r-- root/root       472 2014-04-17 10:24 ./etc/init/cinder-api.conf  -> 프로그램 시작/중지/자동시작 설정
lrwxrwxrwx root/root         0 2014-06-09 15:01 ./usr/share/doc/cinder-api/changelog.Debi


#
# ./usr/bin/cinder-api -> 설치될 메인 프로그램

# root@cinder:/etc/logrotate.d# cat /usr/bin/cinder-api

"""Starter script for Cinder OS API."""

import eventlet
eventlet.monkey_patch()

import os
import sys

from oslo.config import cfg

possible_topdir = os.path.normpath(os.path.join(os.path.abspath(sys.argv[0]),
                                   os.pardir,
                                   os.pardir))
if os.path.exists(os.path.join(possible_topdir, "cinder", "__init__.py")):
    sys.path.insert(0, possible_topdir)

from cinder.openstack.common import gettextutils
gettextutils.install('cinder', lazy=False)

# Need to register global_opts
from cinder.common import config  # noqa
from cinder.openstack.common import log as logging
from cinder import rpc
from cinder import service
from cinder import utils
from cinder import version


CONF = cfg.CONF


if __name__ == '__main__':
    CONF(sys.argv[1:], project='cinder',
         version=version.version_string())
    logging.setup("cinder")
    utils.monkey_patch()

    rpc.init(CONF)
    launcher = service.process_launcher()
    server = service.WSGIService('osapi_volume')
    launcher.launch_service(server, workers=ser

#
# cat /etc/logrotate.d/cinder-api -> cinder-api.log 파일 관리 설정

/var/log/cinder/cinder-api.log {
    daily
    missingok
    compress
    delaycompress
    notifempty
}

#
# ./etc/init/cinder-api.conf  -> 프로그램 시작/중지/자동시작 설정

description "Cinder api server"
author "Chuck Short <zulcss@ubuntu.com>"

start on runlevel [2345]
stop on runlevel [!2345]

chdir /var/run

pre-start script
    mkdir -p /var/run/cinder
    chown cinder:cinder /var/run/cinder

    mkdir -p /var/lock/cinder
    chown cinder:root /var/lock/cinder
end script

exec start-stop-daemon --start --chuid cinder --exec /usr/bin/cinder-api \
     -- --config-file=/etc/cinder/cinder.conf --log-file=/var/log/cinder/cinder-api.log
    
